{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Resilient Distributed Datasets (RDDs) in Apache Spark\n",
        "\n",
        "## What is an RDD?\n",
        "\n",
        "An **RDD** (Resilient Distributed Dataset) is a fundamental data structure in Apache Spark used for:\n",
        "\n",
        "- **Parallel Processing**: Distributing data across multiple nodes for efficient computation.\n",
        "- **Fault Tolerance**: Recovering from failures by recomputing lost data.\n",
        "- **Data Representation**: Handling large-scale data in a distributed manner.\n",
        "\n",
        "RDDs support a variety of **transformations** and **actions**, making them essential for data manipulation and analysis in Spark.\n",
        "\n",
        "## How is an RDD Created?\n",
        "\n",
        "1. **Initialize SparkContext**: Set up the Spark context to begin working with RDDs.\n",
        "2. **Create RDD**: Use methods like `parallelize` or load from external data sources.\n",
        "3. **Perform Transformations and Actions**: Apply operations to manipulate data.\n",
        "4. **Cache RDD**: Optionally cache the RDD for reuse.\n",
        "5. **Handle Data**: Use functions like `filter` or `map` for data processing.\n",
        "6. **Apply Set Theory Operations**: Use operations such as `union` and `intersection` for data manipulation.\n",
        "\n",
        "## Transformations in RDD\n",
        "\n",
        "Transformations are operations that create new RDDs from existing ones. They can be categorized into:\n",
        "\n",
        "- **Narrow Transformations**: Operations where each input partition contributes to only one output partition, e.g., `map`, `filter`. These are fault-tolerant and performed in parallel.\n",
        "- **Wide Transformations**: Operations requiring data shuffling across partitions, e.g., `groupByKey`, `join`. These are more resource-intensive but essential for certain computations.\n",
        "\n",
        "## Actions in RDD\n",
        "\n",
        "Actions are operations that return results or perform computations. They include:\n",
        "\n",
        "- **Actions that Return a Value**: Operations like `reduce` (for aggregating data) and `count` (for counting elements).\n",
        "- **Actions that Return a Unit**: Operations such as `foreach` (for iterating through elements) and `saveAsTextFile` (for saving RDDs to storage).\n",
        "\n",
        "## Lazy Evaluation in RDD\n",
        "\n",
        "**Lazy Evaluation** means that transformations are not executed immediately. Instead, Spark builds a Directed Acyclic Graph (DAG) of transformations, executing them only when an action is called. This optimizes performance and ensures fault tolerance.\n",
        "\n",
        "## Performing Transformations and Actions\n",
        "\n",
        "- **Map Function**: Apply a function to each element to create a new RDD.\n",
        "- **Filter Function**: Extract elements meeting specific criteria.\n",
        "- **Reduce Function**: Aggregate elements to produce a single result.\n",
        "- **Collect Function**: Retrieve all elements of the RDD to the driver.\n",
        "\n",
        "## Common Errors in RDD Operations\n",
        "\n",
        "- **NullPointerExceptions**: Handle null values with conditional checks and `Option` types.\n",
        "- **Out of Memory Errors**: Monitor memory usage, optimize data storage, and partition data.\n",
        "- **Serialization Errors**: Ensure all objects are serializable and consider using Kryo serializer.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Noxt_uTYXzLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SF_PjKxCX4uC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n46NOzdnW2J5",
        "outputId": "a6c70dd2-f466-42cd-8e7f-cbdc91c6b805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=b54fd2f4d257e31152411cb2250a7920dcea65bbc64cb7f15d1cd5822f8e86ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "# to install pyspark using pip\n",
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating RDDs\n",
        "\n",
        "RDDs can be created in several ways:\n",
        "\n",
        "1. **Parallelizing an Existing Collection**: This method involves converting a local collection (like a list) in the driver program into an RDD.\n",
        "\n",
        "2. **Referencing an External Dataset**: You can create an RDD by reading data from external storage systems like HDFS, S3, or local files.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yL6rFkNZYPud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext(\"local\", \"RDD Example\")\n",
        "\n",
        "# Create an RDD from a list of numbers\n",
        "data = [1, 2, 3, 4, 5]\n",
        "rdd = sc.parallelize(data)"
      ],
      "metadata": {
        "id": "5nY17N3pXKrv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply map transformation to square each number\n",
        "squared_rdd = rdd.map(lambda x: x**2)\n",
        "\n",
        "# Print the squared numbers\n",
        "print(\"Squared numbers in RDD:\")\n",
        "for num in squared_rdd.collect():\n",
        "    print(num)\n",
        "\n",
        "# Stop the SparkContext\n",
        "sc.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRic3cdrXU1S",
        "outputId": "1c7d2ad3-ae03-4b62-85f8-af46354fb5b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Squared numbers in RDD:\n",
            "1\n",
            "4\n",
            "9\n",
            "16\n",
            "25\n"
          ]
        }
      ]
    }
  ]
}