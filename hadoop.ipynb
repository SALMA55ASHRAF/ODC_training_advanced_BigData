{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8178ec6-9781-477d-84e6-91cc7c03f1b7",
   "metadata": {},
   "source": [
    "# What is Hadoop Technology?\n",
    "\n",
    "Hadoop is an open-source tool from the ASF — Apache Software Foundation. Open source project means it is freely available and we can even change its source code as per the requirements. If certain functionality does not fulfill your need then you can change it according to your need. Most of Hadoop code is written by Yahoo, IBM, Facebook, Cloudera.\n",
    "\n",
    "It provides an efficient framework for running jobs on multiple nodes of clusters. Cluster means a group of systems connected via LAN. Apache Hadoop provides parallel processing of data as it works on multiple machines simultaneously. Apache Hadoop is an open-source software library that is used to manage data processing and storage in big data applications. Hadoop facilitates analyzing large amounts of data parallelly and more quickly. Apache Hadoop was introduced to the public in 2012 by The Apache Software Foundation(ASF). Google has written a paper on various technologies it uses. It is using technologies like Map-Reduce programming model as well as its file system (GFS). As Hadoop was originally written for the Nutch search engine project. When Doug Cutting and his team were working on it, very soon Hadoop became a top-level project due to its huge popularity. Let us understand Hadoop Definition and Meaning.\n",
    "\n",
    "Apache Hadoop is an open source framework written in Java. The basic Hadoop programming language is Java, but this does not mean you can code only in Java. You can code in C, C++, Perl, Python, ruby etc. as well. You can code the Hadoop framework in any language but it will be more efficient to code in Java as you will have lower level control of the code.\n",
    "\n",
    "Big Data and Hadoop efficiently processes large volumes of data on a cluster of commodity hardware. Hadoop is for processing huge volume of data. Commodity hardware is the low-end hardware, they are cheap devices which are very economical. Hence, Hadoop is very economic.\n",
    "\n",
    "Hadoop can be setup on a single machine (pseudo-distributed mode, but it shows its real power with a cluster of machines. We can scale it to thousand nodes on the fly i.e, without any downtime. Therefore, we need not make any system down to add more systems in the cluster. \n",
    "\n",
    "## Why Apache Hadoop?\n",
    "\n",
    "Before the digital era, the amount of data gathered at a slow pace and could be analyzed and stored with a single storage format. At the same time, the format of the data collected for similar purposes had the same format. However, with the evolution of the Internet and digital platforms like social media, the data comes in various formats (structured, semi-structured, and unstructured) and its velocity also massively increased. A new name was given to this data which is Big Data. Then, the need for multiple processors and storage units arose to handle the big data. Therefore, as a solution Hadoop was introduced.\n",
    "\n",
    "## Hadoop Components\n",
    "\n",
    "Hadoop consists of three main components:\n",
    "\n",
    "1. **HDFS**: The storage unit in Hadoop\n",
    "2. **Map Reduce**: The processing method\n",
    "3. **YARN**: The resource negotiator\n",
    "\n",
    "### HDFS\n",
    "\n",
    "HDFS follows master/slave architecture. It consists of a single namenode and many datanodes. In the HDFS architecture, a file is divided into one or more blocks and stored in separate datanodes. Datanodes are responsible for operations such as block creation, deletion and replication according to namenode instructions. Apart from that, they are responsible to perform read-write operations on file systems.\n",
    "\n",
    "Namenode acts as the master server and the central controller for HDFS. It holds the file system metadata and maintains the file system namespace. Namenode oversees the condition of the datanode and coordinates access to data.\n",
    "\n",
    "**Data replication in HDFS**\n",
    "\n",
    "In HDFS, all blocks consist of the same size except for the last block. An application can specify the number of replicas for a file and the default is 3. The replication factor can be specified at the time of creation and can be changed later on too. Out of the three copies, two copies are stored in different nodes in the same local area network. The third copy will be stored in a different local area network. The namenode is responsible for all the decisions related to block replication. It periodically receives a signal which dictates the health of each node, whether it is functioning properly or not.\n",
    " ![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Eo_clg9z0NUKYh0I2HSR6g.png)\n",
    "\n",
    "### Map Reduce\n",
    "\n",
    "Traditionally, data were processed on a single computer. However, with big data context, it has become a tedious and time consuming task. Therefore Hadoop has its own processing method called Map Reduce. Map Reduce has two tasks, namely Map and Reduce. Mapper is responsible for splitting and mapping the data while the reducer for shuffling and reducing the data.\n",
    "\n",
    "- **Splitting phase**: Input data is split into smaller chunks.\n",
    "- **Mapping phase**: Chunks are converted into `<key, value>` pairs with their occurrence frequency. Here if a word occurs multiple times, it will not state as an accumulated value, but as a single value.\n",
    "- **Shuffling and sorting phase**: The process by which the system performs the sort and sends the output to the reducer. Here the sorting is done based on key not value. Values passed to the reducer can be in any order.\n",
    "- **Reducing phase**: Once the shuffling and sorting is done, the reducer combines the result and outputs it which is stored in HDFS.\n",
    "![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*b1OVW3wsS8KXy6eduf9qrQ.png)\n",
    "\n",
    "### YARN\n",
    "\n",
    "YARN stands for “Yet Another Resource Negotiator”. YARN is responsible for resource management and job scheduling in HDFS. YARN architecture has the following components:\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*_d4kWQZSc8R3N6zBB8Rrng.png)\n",
    "\n",
    "1. **Client**: Submits map-reduce jobs\n",
    "2. **Resource Manager**: Responsible for resource assignment and management across the application. When a map-reduce task is received, it forwards the task to the respective node manager, and does the resource allocation accordingly. Resource manager consists of two main components namely scheduler and application manager.\n",
    "   - **Scheduler**: Performs scheduling based on the allocated application and available resources.\n",
    "   - **Application Manager**: Responsible for negotiation of resources with the resource manager, monitoring the application progress and application status tracking.\n",
    "3. **Node Manager**: Takes care of an individual node on Hadoop cluster and its application workflow. Responsible for creating the container process and also killing as per the instructions given by the resource manager. In addition, it is also responsible for monitoring resource usage and performing log management.\n",
    "4. **Application Master**: Responsible for negotiating resources with the resource manager. This tracks the status and monitors the progress of a single application. Application master requests the container from the node manager by sending the relevant details to run the application and once it runs a time-to-time report about the health of the container to the resource manager.\n",
    "5. **Container**: A collection of physical records such as RAM, CPU cores and disk on a single node.\n",
    "\n",
    "## What is Hadoop Architecture?\n",
    "\n",
    "![Hadoop Architecture](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*6_OVFpo95F9PwntQRqMiAA.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0142c21-6975-4e75-8946-06a538300770",
   "metadata": {},
   "source": [
    "Hadoop works in a master-slave fashion. There is a master node and there are `n` number of slave nodes, where `n` can be in the thousands. The master manages, maintains, and monitors the slaves while the slaves are the actual worker nodes. In Hadoop architecture, the master should be deployed on high configuration hardware, not just commodity hardware, as it is the centerpiece of the Hadoop cluster.\n",
    "\n",
    "The master stores the metadata (data about data), while the slaves are the nodes that store the actual data. Data is distributed across the cluster. The client connects with the master node to perform any task. In this Hadoop for beginners tutorial, we will discuss the different components of Hadoop in detail.\n",
    "\n",
    "## 5. Hadoop Components\n",
    "\n",
    "There are three most important Apache Hadoop components. In this tutorial, you will learn what HDFS is, what Hadoop MapReduce is, and what Yarn Hadoop is. Let us discuss them one by one:\n",
    "\n",
    "### 5.1. What is HDFS?\n",
    "\n",
    "Hadoop HDFS (Hadoop Distributed File System) is a distributed file system that provides storage in Hadoop in a distributed fashion.\n",
    "\n",
    "In Hadoop architecture, on the master node, a daemon called **namenode** runs for HDFS. On all the slave nodes, a daemon called **datanode** runs. Hence, slaves are also called datanodes. The namenode stores metadata and manages the datanodes. On the other hand, datanodes store the data and perform the actual tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0145f50-ebbb-40d6-a714-b4907a9b2225",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:2000/format:webp/0*qBmynxJEwZ2sNSaq.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14327cc8-f5e7-44dc-b630-c32a02f7884a",
   "metadata": {},
   "source": [
    "HDFS is a highly fault-tolerant, distributed, reliable, and scalable file system for data storage. First, follow this guide to learn more about the features of HDFS and then proceed further with the Hadoop tutorial.\n",
    "\n",
    "HDFS is developed to handle huge volumes of data. The file size expected is in the range of GBs to TBs. A file is split up into blocks (default size 128 MB) and stored distributedly across multiple machines. These blocks replicate according to the replication factor. After replication, the blocks are stored at different nodes. This mechanism handles the failure of a node in the cluster. \n",
    "\n",
    "For example, if there is a file of 640 MB, it is broken down into 5 blocks of 128 MB each (using the default block size).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf8a534-94a7-4513-b7f2-6b64a41fbc19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
